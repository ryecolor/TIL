# pjt-02. Data Science

<br>

## Index
0. 금주 학습 내용
1. 데이터 전처리 - 데이터 읽어오기
2. 데이터 전처리 - 2021년 이후의 종가 데이터 출력
3. 데이터 분석 - 2021년 이후 최고, 최저가 출력
4. 데이터 분석 - 2021년 이후 월별 평균 종가 출력
5. 데이터 시각화 - 2022년 1월 이후 월별 최고, 최저, 종가 시각화
6. 생성형 AI 활용 자율 학습 - 주가 예측
7. 학습 이후 느낀 점

<br>
<br>

> 프로젝트 목표

- `데이터 사이언스 분야`에 대해 이해하기

- 데이터 사이언스에서 자주 사용되는 `파이썬 패키지`를 사용해 보기

<br>

### 0. 금주 학습 내용
---

<br>

> 학습 내용 정리

1. Google 주식 데이터 다운로드

2. 데이터 사이언스에서 자주 사용되는 패키지 사용

3. 원하는 데이터만 뽑아 내어 차트 출력

---

- 진행 순서

  - 데이터 사이언스 기초 이론 학습

  - 데이터들이 모여 있는 사이트 `캐글(Kaggle)`에서 실습 데이터 다운로드
  
    - 구글, 넷플릭스 주가 데이터
  
  - 데이터 사이언스에서 자주 쓰이는 패키지 학습

    - `Numpy`, `Pandas`, `Matplotlib`

---

- 개발 도구

  - Python 3.9+

  - `Jupyter notebook`

    - 데이터 사이언스 작업에 많이 활용되는 파이썬 개발 환경
    
    - 웹 브라우저에서 실행
    
    - 코드 실행, 텍스트 문서 작성, 시각화 등을 하나의 문서에 통합하여 작업 가능
    
    - 데이터 사이언스 작업에 많이 쓰이는 이유

      - `셀 단위 코드 실행`으로 결과 확인이 용이하며 연구에 최적화
      
      - 문서를 작성할 수 있는 `마크다운 기능` 제공
      
      - `개별적인 코드 블록` 실행 가능

---

- 데이터 사이언스

<br>

![image](https://postfiles.pstatic.net/MjAyNDA3MjZfMTM0/MDAxNzIxOTUzMDM5NzI0.PkFhnGe3fwTl5EgKprG8-hRJBzSv1KVNZzR8zWLcXB0g.rEdFKJwvq_vTOZaoErCwDj2HMzuRiL0fU_mzc_wxGAAg.PNG/%EA%B7%B8%EB%A6%BC1.png?type=w773)

<br>

  - `다양한 데이터로부터 새로운 지식과 정보를 추출`하기 위해 과학적 방법론, 프로세스, 알고리즘, 시스템을 동원하는 융합 분야

  - 컴퓨터 과학, 통계학, 수학 등 다양한 학문의 원리와 기술을 활용

<br>

> 데이터 사이언스 프로세스

- 필요한 정보를 추출하는 5단계

1. `문제 정의` : 해결하고자 하는 문제 정의

2. `데이터 수집` : 문제 해결에 필요한 데이터 수집

3. `데이터 전처리(정제)` : 실질적인 분석을 수행하기 위해 데이터를 가공하는 단계

   - 수집한 데이터의 오류 제거(결측지, 이상치), 데이터 형식 변환 등

4. `데이터 분석` : 전처리가 완료된 데이터에서 필요한 정보를 추출하는 단계 → AI 주로 활용

5. `결과 해석 및 공유` : 의사 결정에 활용하기 위해 결과를 해석하고 시각화 후 공유하는 단계

<br>

> 데이터 수집 방법

- `웹 스크래핑` (Web Scraping)

- `웹 크롤링`​ (Web Crawling)

- `Open API` 활용

- `데이터 공유 플랫폼` 활용: `Kaggle`, Data.world, Dacon, 공공데이터포털 등

    **Google에서 robots.txt 검색 시 적법한 크롤링 범위 확인 가능*

<br>

> 캐글 ([Kaggle](https://www.kaggle.com/))

- 기업 및 단체에서 데이터와 해결 과제를 등록하면, 데이터 과학자들이 이를 해결하는 방법을 개발하고 경쟁할 수 있는 `데이터 분석 경진대회 플랫폼`

- 경진대회, 데이터셋 공유, 토론 등의 기능 활용 가능

<br>

> CSV란?

- 몇 가지 필드를 `쉼표(,)`로 구분한 `텍스트`​​ 데이터 및 텍스트 파일

- 일반적으로 `표 형식의 데이터`를 CSV 형태로 많이 사용

- 저장, 전송 및 `처리 속도`가 빠르며, 다양한 프로그램 처리 가능 (`호환성` 높음)

<br>

> 데이터 처리 및 분석 파이썬 패키지

- Numpy

    - 수학 계산용 패키지

    - Pandas와 Matplotlib를 사용하기 위해 활용되는 패키지
    
    - `다차원 배열`을 쉽게 처리하고 효율적으로 사용할 수 있도록 지원

    - 장점
        ``` python
        1. Numpy 행렬 연산은 데이터가 많을수록 Python 반복문보다 '빠름'
        2. '다차원 행렬 자료 구조'를 제공하여 개발하기 편리
        ```

    - 특징
        ``` python
        1. 공식 사이트의 Python('CPython')에서만 사용 가능
        2. '행렬 인덱싱(Array Indexing)' 기능 제공
        ```
    
    - 실습 파일
        
        1.Numpy_Basic.ipynb

<br>

- Pandas

    - 원하는 데이터만 추출하거나 데이터를 분석할 때 활용되는 패키지

    - Numpy의 한계

        ``` python
        1. '유연성': 데이터에 레이블을 붙이거나, 누락된 데이터로 작업이 부족
        
        2. '구조화 어려움': 그룹화, 피벗 등 기능 부족
        ```
    
    - Pandas는 `프로그래밍 버전의 엑셀`을 다루듯 고성능 데이터 구조 제작 가능
    
    - Numpy 기반으로 만들어진 패키지로, Series(1차원 배열)와 DataFrame(2차원 배열)이라는 효율적인 자료 구조 제공
    
    - 실습 파일
    
        2.Pandas_Basic.ipynb / 3.Pandas_Advanced.ipynb

<br>

- Matplotlib

    - 그래프를 그려 주는 `데이터 시각화 패키지`

    - Python에서 데이터 시각화를 위해 가장 널리 사용되는 라이브러리

    - 다양한 종류의 그래프와 도표를 생성하고 데이터를 시각적으로 표현 가능

    - 실습 파일

        4.matplotlib_basic.ipynb

<br>
<br>

### 1. `데이터 전처리` - 데이터 읽어오기
---

<br>

> Pandas를 사용하여 `csv 파일(NLFX.csv)을 DataFrame으로` 읽어 옵니다. 이때 `['Date', 'Open', 'High', 'Low', 'Close'] 필드만` 읽어 오도록 구성합니다.

<br>

#### ① 필요 패키지 import
---
데이터를 읽어 오고 전처리, 분석, 시각화의 과정을 거치기 위해서는 앞서 언급된 `Numpy`, `Pandas`, `Matplotlib`과 같은 패키지를 사용해야 한다.

특히 위 세 패키지는 자주 사용되므로 `as를 통해 별칭을 지정`해 준다.

``` python
# 패키지 import
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
```

<br>

#### ② csv 파일 읽어 오기
---
Pandas에서 지원하는 csv 데이터 파일을 읽어 오는 함수 `read_csv()`를 사용한다.

이때 문제 요구 사항을 위해 필요한 특정 열만 가져오기 위해 해당 함수의 변수 `usecols`를 지정한다.

``` python
# csv 파일 경로 지정
csv_path = "archive/NFLX.csv"

# 필요한 데이터만 읽어오기
df = pd.read_csv(csv_path, usecols= ['Date', 'Open', 'High', 'Low', 'Close'])

# 데이터프레임 확인하기
df
```

#### ③ NFLX 데이터 설명
---
- `Date` : 거래일
- `Open` : 당일 개시가
- `High` : 당일 최고가
- `Low` : 당일 최저가
- `Close` : 당일 종가

``` python
# 데이터 타입 확인
df.dtypes

# Date      object
# Open     float64
# High     float64
# Low      float64
# Close    float64
# dtype: object
```

<br>
<br>

### 2. `데이터 전처리` - 2021년 이후의 종가 데이터 출력
---

<br>

> csv 파일을 DataFrame으로 읽어와 `2021년 이후의 종가 데이터만 필터링`한 후 `Matplotlib를 사용하여 시각화`합니다.
> - 필터링이 가능한 형식으로 데이터 타입을 변경
> - Pandas의 `to_datetime()` 활용

<br>

#### ① 데이터 타입 변환
---
문제 1번과 같은 방식으로 데이터 프레임을 읽어 온다.

단, 기존 데이터 프레임에서 Date 항목의 데이터 타입은 필터링이 불가한 `object(문자열)`이었으므로 이를 `datetype64`로 변경하는 `to_datetime()` 함수를 사용한다.

<br>

``` python
# CSV 파일 경로
csv_path = "archive/NFLX.csv"

# 1단계: 특정 필드만 필터링
df = pd.read_csv(csv_path, usecols= ['Date', 'Close'])

# 2단계: Date의 데이터 타입 변경
df['Date'] = pd.to_datetime(df['Date'])

# 3단계: Date의 타입 확인
df.dtypes

  # Date     datetime64[ns]
  # Close           float64
  # dtype: object
```

<br>

#### ② 데이터 필터링
---
개인적으로는 월별 데이터가 기록되지 않는 `연도별 데이터 필터링`으로 진행했다. 따라서 추후 확인한 `연도 열을 새로 생성하지 않고 필터링을 진행`하는 방식도 함께 기록하였다.

<br>

- `연도 열 생성` 후 필터링

  ``` python
  # 'Date' 복제 열 생성
  df['Year'] = pd.to_datetime(df['Date'])

  # 복제 열에서 year 정보만 남기기
  df1 = df['Year'].dt.year
  df['Year'] = df1

  # year의 값이 2021 이상인 데이터만 필터링
  idxs = df[df['Year'] < 2021].index
  df.drop(idxs, inplace=True)
  ```

- `즉시 필터링` 진행

  ``` python
  df = df[df['Date'] >= pd.to_datetime('2021-01-01')]
  ```

<br>

#### ③ 데이터 그래프 시각화
---
matplotlib를 활용하여 데이터 기반의 그래프를 시각화하였다. 이때, 본 답변에서는 사용하지 않았으나 각 항목의 글자가 서로 겹침이 없도록 조정하는 `plt.xticks(rotation=45)` 등의 설정이 가능하다.

<br>

``` python
import matplotlib.pyplot as plt

# 데이터 생성
x = df['Date']  # x 좌표값
y = df['Close']  # y 좌표값

# 그래프 그리기
plt.plot(x, y)

# 그래프에 제목과 축 레이블 추가
plt.title('NFLX Close Price')
plt.xlabel('Date')
plt.ylabel('Close Price')

# 그래프 표시
plt.show()
```

<br>
<br>

### 3. `데이터 분석` - 2021년 이후 최고, 최저가 출력
---

<br>

> 2021년 이후의 데이터 내 종가 필드를 활용하여, 최고가와 최저가를 출력합니다.
> - `Pandas의 내장 함수` 사용

<br>

#### ① `sort_values()`와 `loc()` 활용하기
---
우선 개인적으로 종가(Close) 필드에서 최고가와 최저가를 출력하기 위해 종가를 기준으로 sort를 진행한 후, 각각의 행을 따로 출력하는 방식으로 진행하였다.

``` python
# 종가 기준으로 sort
df_sort = df.sort_values(by = 'Close')

# 최고 종가, 최저 종가 변수 저장
max_price = df.loc[954]['Close']
min_price = df.loc[1001]['Close']

# 최고 종가, 최저 종가 출력
print("최고 종가:", max_price)
print("최저 종가:", min_price)
```

<br>

#### ② `max()`와 `min()` 활용하기
---
실습 이후 솔루션을 확인하며 해당 함수들을 다시 떠올렸다. 최고가와 최저가를 찾는 더욱 최적화된 방법은 이 방식인 것 같다.

``` python
# 최고가 출력
max_price = df['Close'].max()
print("최고 종가:", max_price)

# 최저가 출력
min_price = df['Close'].min()
print("최저 종가:", min_price)
```

<br>

두 가지 방식 모두 다음과 같은 결과를 얻을 수 있다.

> 최고 종가: 691.690002

> 최저 종가: 359.700012

<br>
<br>

### 4. `데이터 분석` - 2021년 이후 월별 평균 종가 출력
---

<br>

> 2021년 이후의 데이터를 `월별로 그룹화`하여 `평균 종가`를 계산한 `새로운 DataFrame`을 만들어 그래프로 시각화합니다.

<br>

#### ① `연도`와 `달`을 표기하는 새로운 열 생성
---
가장 먼저 월별 데이터를 얻기 위해 앞서 연도만 나타내는 year 열을 생성했던 것처럼 `dt.strftime('%Y-%m')`으로 연도와 달을 나타내는 year_month 열을 새로 생성하였다.

이후 해당 열을 기준으로 `groupby()` 함수와 `mean()`을 통해 평균 종가를 계산하였으며, `reset_index()`를 곁들여 새로운 데이터 프레임 montly_close로 저장하였다.

``` python
#연도와 달을 표기하는 새로운 열 생성
df['Date'] = pd.to_datetime(df['Date'])
df['year_month'] = df['Date'].dt.strftime('%Y-%m')

# 월별 평균 종가 출력하는 새로운 데이터프레임 생성 및 확인
monthly_close = df.groupby('year_month')['Close'].mean().reset_index()
monthly_close
```

<br>

#### ② 월별 평균 종가 그래프 시각화
---
기존과 동일한 방식으로 시각화한다. 이번에는 확연히 레이블 텍스트의 길이가 길어졌으므로 `figsize` 등의 값 조정을 통해 그래프의 크기와 레이아웃을 수정하였다.

<br>

``` python
# 데이터 생성
x = monthly_close['year_month'] # x 좌표값
y = monthly_close['Close'] # y 좌표값

# 그래프 크기 설정
plt.figure(figsize=(12, 6))

# 그래프 그리기
plt.plot(x, y)

# x축 레이블 조정
plt.xticks(rotation=45, ha='right')  # 레이블을 45도 회전하고 오른쪽 정렬

# 그래프에 제목과 축 레이블 추가
plt.title('Monthly Average Close Price')
plt.xlabel('Date')
plt.ylabel('Average Close Price')

# 그래프 레이아웃 조정
plt.tight_layout()

# 그래프 표시
plt.show()
```

<br>

실습 솔루션에서는 다른 방식으로 월별 평균 종가를 계산하였다. 개인적으로 사용한 dt.strftime() 대신 `dt.to_period()`를 사용하였으며, `numeric_only`의 값을 명시적으로 설정한 것이 특징이다.

groupby() 함수로 인해 행으로 변경된 열을 다시 `reset_index`를 통해 되돌린 것은 동일했다. 이후 'Date' 열의 형식으로 다시 datetime으로 돌리기 위해서는 dt.to_timestamp()를 사용하였다.

``` python
# 월별 평균 종가 계산 (numeric_only=True를 명시적으로 설정)
monthly_avg_close = df.groupby(df['Date'].dt.to_period('M')).mean(numeric_only=True)

# Date 열이 행으로 변경되었으므로, reset_index를 통해 열로 다시 변경
monthly_avg_close.reset_index(inplace=True)

# 'Date' 컬럼을 datetime 형식으로 변환
monthly_avg_close['Date'] = monthly_avg_close['Date'].dt.to_timestamp()
```

<br>
<br>

### 5. `데이터 시각화` - 2022년 1월 이후 월별 최고, 최저, 종가 시각화
---

<br>

> 2022년 이후의 데이터만 필터링하여 `최고, 최저, 종가의 3가지 필드`를 한번에 분석 가능하도록 시각화합니다.

<br>

앞선 실습에서 사용하지 않은 열의 데이터가 필요하므로 다시 데이터를 가져와 필터링하는 과정을 거쳤다. 이번에는 연도가 2022 이상의 값을 가지는 데이터만 남겼다.

``` python
# CSV 파일 경로
csv_path = "archive/NFLX.csv"

# 2021년 이후의 종가 데이터 출력하기
# 1단계: 필요 요소만 필터링
df = pd.read_csv(csv_path, usecols= ['Date', 'High', 'Low', 'Close'])

# 2단계: Date의 타입이 object이므로 필터링 가능한 형식으로 변경
df['Date'] = pd.to_datetime(df['Date'])

# 3단계: 연도만 추출한 새로운 열 생성
df['Year'] = pd.to_datetime(df['Date'])
df1 = df['Year'].dt.year
df['Year'] = df1

# 4단계: Date의 연도가 2022 이상인 것만 필터링
idxs = df[df['Year'] < 2022].index
df.drop(idxs, inplace=True)
```

이후, 이번에는 3가지 필드 값이 모두 그래프로 시각화되어야 하므로 y의 값을 늘려 다음과 같은 방식으로 모두 기입하였다.

``` python
# 데이터 생성
x = df['Date']  # x 좌표값
y = df['High'] # y 좌표값
y2 = df['Low'] # y2 좌표값
y3 = df['Close'] # y3 좌표값

# 그래프 크기 설정
plt.figure(figsize=(12, 6))

# 그래프 그리기
plt.plot(x, y, label='High')
plt.plot(x, y2, label='Low')
plt.plot(x, y3, label='Close')

# x축 레이블 조정
plt.xticks(rotation=45, ha='right')  # 레이블을 45도 회전하고 오른쪽 정렬

# 범례 추가
plt.legend()

# 그래프에 제목과 축 레이블 추가
plt.title('High, Low, and Close Prices since January 2022')
plt.xlabel('Date')
plt.ylabel('Price')

# 그래프 레이아웃 조정
plt.tight_layout()

# 그래프 표시
plt.show()
```

<br>
<br>


### 6. `생성형 AI` 활용 자율 학습 - 주가 예측
---

<br>

> 생성형 AI가 주가를 예측하도록 프롬프트를 구성해 봅니다.

<br>

금번 실습에서는 주로 사용하는 ChatGPT가 아닌 생성형 AI 서비스 `Perplexity`를 사용하였다.

유료 서비스가 아닌 만큼, 파일이나 이미지를 입력하는 유료 기능을 사용하지 않은 프롬프트는 `변화하는 동향만을 설명`하는 추상적인 형태였으나, AI는 즉시 입력만으로 `연관 정보를 추가 탐색`하여 고퀄리티의 답변을 출력하였다. 결과는 nflx_analysis 파일을 통해 확인할 수 있다.

<br>
<br>

### 7. 학습 이후 `느낀 점`
---

<br>

- `Numpy`, `Pandas`, `Matplotlib`의 기능을 익히고 실제로 사용해 보는 경험을 할 수 있었습니다.

- `Pandas`의 다양한 내장 함수만으로 쉽게 데이터를 정제하고, `Matplotlib`를 통한 그래프 시각화까지 성공할 수 있었습니다.

- 하나의 코드 블럭씩 실행하여 결과를 확인할 수 있다는 점에서 `Jupyter Notebook`의 효능을 확인할 수 있었습니다.

- 문제 6번 외에도 사용해야 할 올바른 Pandas 코드 구성을 위해 `생성형 AI`를 적극적으로 활용하였습니다. 인터넷상의 정보를 탐색하는 과정의 효율성을 극도로 높일 수 있었으며, 이는 곧 시간 단축으로 이어졌습니다.